{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65992ef3-0b19-43da-9c41-1b1f76fa93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Contingency Matrix in Classification Evaluation:\n",
    "A contingency matrix (also known as a confusion matrix) is a table that visualizes the performance of a classification model by comparing predicted versus actual class labels. It has rows for actual classes and columns for predicted classes, where each cell shows the count of samples falling into that category.\n",
    "\n",
    "\n",
    "Q4. Intrinsic versus Extrinsic Measure:\n",
    "Intrinsic Measures are general metrics that evaluate a model's performance without considering a specific task. In natural language processing, an intrinsic measure could be perplexity in language modeling.\n",
    "Extrinsic Measures focus on a model's performance in the context of a particular task, showing how well the model performs in real-world applications.\n",
    "Q5. Purpose of a Confusion Matrix in Machine Learning:\n",
    "Identifying Model Performance: It helps understand the model's accuracy, recall, precision, and F1 score.\n",
    "Strengths and Weaknesses Identification: Allows assessment of a model's ability to correctly classify and the types of errors it makes.\n",
    "Q6. Intrinsic Measures for Unsupervised Learning:\n",
    "Inertia or Within-Cluster Sum of Squares: Measures compactness of clusters in K-means clustering.\n",
    "Silhouette Score: Assesses the separation distance between clusters.\n",
    "Q7. Limitations of Using Accuracy as the Sole Evaluation Metric:\n",
    "Imbalanced Datasets: Accuracy might not reflect the model's actual performance when classes are imbalanced.\n",
    "Misrepresentation of Errors: Ignores the type of errors a model makes.\n",
    "Unsuitable for Multi-Class Problems: Might not reflect well on multi-class classification problems.\n",
    "Addressing Limitations:\n",
    "Using precision, recall, F1-score, ROC curve, or AUC-ROC for a more comprehensive view.\n",
    "Balancing classes or using alternative metrics suited to the problem at hand.\n",
    "Utilizing a mix of evaluation metrics provides a more comprehensive view of a model's performance, addressing its strengths and limitations in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc370d74-c98d-4fb8-b33b-6b0263c661f2",
   "metadata": {},
   "source": [
    "Q2. Pair Confusion Matrix versus Regular Confusion Matrix:\n",
    "Regular Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "Pair Confusion Matrix: Extends the regular confusion matrix for multi-label classification, considering pairwise combinations of classes. It's useful in multi-label scenarios where individual class performance is less critical than class pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd3825-3e38-4647-acdc-665fb42cce10",
   "metadata": {},
   "source": [
    "Q3. Extrinsic Measure in Natural Language Processing:\n",
    "Extrinsic measures evaluate the performance of language models in the context of a specific task. For instance, in natural language processing, it might involve evaluating a language model in the context of sentiment analysis, machine translation, or named entity recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a319736-662d-4d8f-b94e-e9c1b6a32bb5",
   "metadata": {},
   "source": [
    "Q4. Intrinsic versus Extrinsic Measure:\n",
    "Intrinsic Measures are general metrics that evaluate a model's performance without considering a specific task. In natural language processing, an intrinsic measure could be perplexity in language modeling.\n",
    "Extrinsic Measures focus on a model's performance in the context of a particular task, showing how well the model performs in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06047938-6bac-4a47-94fa-f097e2a3e146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916a140-e0eb-44b0-ad34-667a3d056f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94138df1-1452-40d5-bc5e-cffede72096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa551df-17f9-4ecb-93e2-56d6facf2610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78cd26-61b2-44f7-b595-b9697517c84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
